from __future__ import annotations

import asyncio

from agents import Agent, ItemHelpers, Runner, TResponseInputItem, trace

# Agents
prompt_generator_agent = Agent(
    name="Prompt Generator Agent",
    instructions="""
    You are a Prompt Generator Agent.
    Your task is to transform vague or broad user instructions into clear, actionable instructions that can be processed by other agents. Use the examples below to guide your improvements:

    Example 1:
    Worse: How do I add numbers in Excel?
    Better: How do I add up a row of dollar amounts in Excel? I want to do this automatically for a whole sheet of rows with all the totals ending up on the right in a column called "Total".

    Example 2:
    Worse: Whoâ€™s president?
    Better: Who was the president of Mexico in 2021, and how frequently are elections held?

    Example 3:
    Worse: Write code to calculate the Fibonacci sequence.
    Better: Write a TypeScript function to efficiently calculate the Fibonacci sequence. Comment the code liberally to explain what each piece does and why it's written that way.

    Example 4:
    Worse: Summarize the meeting notes.
    Better: Summarize the meeting notes in a single paragraph. Then write a markdown list of the speakers and each of their key points. Finally, list the next steps or action items suggested by the speakers, if any.
    
    Follow prompt engineering best practices to refine the initial instructions into something more specific and actionable for LLMs.
    Provide only the improved instructions.
    """,
)
evaluator_agent = Agent(
    name="Evaluator Agent",
    instructions="""
    You are an Evaluator Agent. Imagine you are a seasoned professor in a prestigious university, known for your critical eye and constructive feedback.
    Your task is to meticulously evaluate the quality of the instructions generated by the prompt generator agent, ensuring they meet the highest standards of clarity and specificity.
    Select the best instructions from the list of generated instructions.
    """,
    model="o3-mini",
)


async def main() -> None:
    msg = input("What instructions would you like to refine? ")
    input_items: list[TResponseInputItem] = [{"content": msg, "role": "user"}]

    # Run the entire workflow in a single trace
    with trace("Generating Better Instructions..."):
        # Run all the agents with the instructions generated by the prompt generator agent
        result_1, result_2, result_3, result_4, result_5 = await asyncio.gather(
            Runner.run(prompt_generator_agent, input_items),
            Runner.run(prompt_generator_agent, input_items),
            Runner.run(prompt_generator_agent, input_items),
            Runner.run(prompt_generator_agent, input_items),
            Runner.run(prompt_generator_agent, input_items),
        )
        outputs = [
            ItemHelpers.text_message_outputs(result_1.new_items),
            ItemHelpers.text_message_outputs(result_2.new_items),
            ItemHelpers.text_message_outputs(result_3.new_items),
            ItemHelpers.text_message_outputs(result_4.new_items),
            ItemHelpers.text_message_outputs(result_5.new_items),
        ]
        generated_instructions = "\n\n".join(outputs)

        # Evaluate the instructions
        best_instructions = await Runner.run(
            evaluator_agent,
            f"Input: {msg}\n\nGenerated Instructions:\n{generated_instructions}",
        )

    print("\n\n--------------------------------")
    print(f"Generated Instructions: {generated_instructions}")
    print(f"Best Instructions: {best_instructions.final_output}")
    print("--------------------------------\n\n")


if __name__ == "__main__":
    asyncio.run(main())
